{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-14 21:31:46.480494\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(datetime.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scrape State Court data (past 3 months)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court = Court('supreme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court.pull_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1) Fetch the State Court content by URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Target State Court page:\n",
    "url = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=subordinate\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print(response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2) Parse the HTML document with Beautiful Soup.\n",
    "\n",
    "This step allows us to access the elements of the document by XPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_court = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05 Apr 2021'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "court_full = pd.read_csv(f'../data/statecourt_compiled.csv')\n",
    "last_entry = court_full.title[0]\n",
    "last_date = court_full.date[0]\n",
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code collects the date, name, and links for judgments from the High Court \n",
    "# which are accessible on available on LawNet\n",
    "\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "# Create variable for the domain\n",
    "domain = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=1&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=openContentPage&_freeresources_WAR_lawnet3baseportlet_docId=\"\n",
    "\n",
    "# Get number of pages\n",
    "# Remove all extra strings to get the number\n",
    "page = str(state_court.find_all('li', {'class': 'lastPageActive'}))\n",
    "page = ''.join(filter(str.isdigit, page))\n",
    "# Replace the first \"3\" as it comes from the url\n",
    "last_page = int(page.replace(\"3\",\"\",1))\n",
    "\n",
    "# Create counter for current page\n",
    "current_page = 1\n",
    "# Loop while it is not the last page\n",
    "while current_page <= last_page:\n",
    "    url1 = url+\"&_freeresources_WAR_lawnet3baseportlet_page=\"+str(current_page)\n",
    "    # Establishing the connection to the web page:\n",
    "    response1 = requests.get(url1)\n",
    "    # Pull the HTML string out of requests and convert it to a Python string.\n",
    "    html1 = response1.text\n",
    "    state_court1 = BeautifulSoup(html1, 'lxml')\n",
    "    # Get the relevant elements (date, name, link)\n",
    "    search_results = state_court1.find_all('ul', {'class': 'searchResultsHolder'})\n",
    "    for li in search_results:\n",
    "        li_list = li.find_all('li')\n",
    "        li_list.pop(0)\n",
    "        for element in li_list:\n",
    "            # start a dictionary to store this item's data\n",
    "            result = {}\n",
    "            # get the date\n",
    "            date = element.find('p', {'class': 'resultsDate'}).text\n",
    "            result['date'] = date\n",
    "            # get the title and full link/url\n",
    "            a_href = element.find('a')\n",
    "            text = a_href.text\n",
    "            if text == last_entry and date == last_date:\n",
    "                \n",
    "            # only store \"full\" rows of data\n",
    "            elif a_href:\n",
    "                result['title'] = a_href.text.strip()   # element text\n",
    "                link = str(a_href['href']) # href link\n",
    "                link = link.replace(\"javascript:viewContent\",\"\")\n",
    "                link = link.strip(\"')(\")\n",
    "                result['link'] = domain+link\n",
    "            if len(result) == 3:\n",
    "                results_list.append(result)\n",
    "\n",
    "               \n",
    "    # Raise page counter\n",
    "    current_page += 1\n",
    "state_court_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow to only Criminal cases\n",
    "state_court_df = state_court_df[state_court_df['title'].str.contains(\"Public Prosecutor\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1) Fetch the Supreme Court content by URL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Target Supreme Court page:\n",
    "url = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=supreme\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print(response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2) Parse the HTML document with Beautiful Soup.\n",
    "\n",
    "This step allows us to access the elements of the document by XPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code collects the date, name, and links for judgments from the High Court \n",
    "# which are accessible on available on LawNet\n",
    "\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "# Create variable for the domain\n",
    "domain = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=1&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=openContentPage&_freeresources_WAR_lawnet3baseportlet_docId=\"\n",
    "\n",
    "# Get number of pages\n",
    "# Remove all extra strings to get the number\n",
    "page = str(supreme_court.find_all('li', {'class': 'lastPageActive'}))\n",
    "page = ''.join(filter(str.isdigit, page))\n",
    "# Replace the first \"3\" as it comes from the url\n",
    "last_page = int(page.replace(\"3\",\"\",1))\n",
    "\n",
    "# Create counter for current page\n",
    "current_page = 1\n",
    "# Loop while it is not the last page\n",
    "while current_page <= last_page:\n",
    "    url1 = url+\"&_freeresources_WAR_lawnet3baseportlet_page=\"+str(current_page)\n",
    "    # Establishing the connection to the web page:\n",
    "    response1 = requests.get(url1)\n",
    "    # Pull the HTML string out of requests and convert it to a Python string.\n",
    "    html1 = response1.text\n",
    "    supreme_court1 = BeautifulSoup(html1, 'lxml')\n",
    "    # Get the relevant elements (date, name, link)\n",
    "    search_results = supreme_court1.find_all('ul', {'class': 'searchResultsHolder'})\n",
    "    for li in search_results:\n",
    "        li_list = li.find_all('li')\n",
    "        for element in li_list:\n",
    "            # start a dictionary to store this item's data\n",
    "            result = {}\n",
    "            # get the date\n",
    "            result['date'] = element.find('p', {'class': 'resultsDate'}).text\n",
    "            # get the title and full link/url\n",
    "            a_href = element.find('a')\n",
    "            if a_href:\n",
    "                result['title'] = a_href.text   # element text\n",
    "                link = str(a_href['href']) # href link\n",
    "                link = link.replace(\"javascript:viewContent\",\"\")\n",
    "                link = link.strip(\"')(\")\n",
    "                result['link'] = domain+link\n",
    "            # only store \"full\" rows of data\n",
    "                if len(result) == 3:\n",
    "                    results_list.append(result)\n",
    "    # Raise page counter\n",
    "    current_page += 1\n",
    "supreme_court_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow to only Criminal cases\n",
    "supreme_court_df = supreme_court_df[supreme_court_df['title'].str.contains(\"Public Prosecutor\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compiled databases\n",
    "state_court_full = pd.read_csv('../data/statecourt_compiled.csv')\n",
    "supreme_court_full = pd.read_csv('../data/supremecourt_compiled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with compiled databases and only save new entries\n",
    "state_court_df = state_court_df[~state_court_df['link'].isin(state_court_full['link'])]\n",
    "supreme_court_df = supreme_court_df[~supreme_court_df['link'].isin(supreme_court_full['link'])]\n",
    "\n",
    "state_court_full = state_court_df.merge(state_court_full, how='outer')\n",
    "supreme_court_full = supreme_court_df.merge(supreme_court_full, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/statecourt_2021-04-13.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f57f3ba9f846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Export to .csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstate_court_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'../data/statecourt_{today}.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msupreme_court_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'../data/supremecourt_{today}.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda4\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda4\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda4\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/statecourt_2021-04-13.csv'"
     ]
    }
   ],
   "source": [
    "# Export to .csv\n",
    "state_court_df.to_csv(path_or_buf=f'../data/statecourt.csv', index=False)\n",
    "supreme_court_df.to_csv(path_or_buf=f'../data/supremecourt.csv', index=False)\n",
    "\n",
    "state_court_full.to_csv(path_or_buf=f'../data/statecourt_compiled.csv', index=False)\n",
    "supreme_court_full.to_csv(path_or_buf=f'../data/supremecourt_compiled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48 entries, 1 to 71\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    48 non-null     object\n",
      " 1   title   48 non-null     object\n",
      " 2   link    48 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "state_court_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following Links for More Results\n",
    "\n",
    "One hundred results is pretty good, but what if we want more? We need to follow the \"next\" links and find new pages to grab. Using the **`parse()`** method of our spider class, we need to return another type of object.\n",
    "\n",
    "See [Stack Overflow](https://stackoverflow.com/questions/30152261/make-scrapy-follow-links-and-collect-data) for details!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
