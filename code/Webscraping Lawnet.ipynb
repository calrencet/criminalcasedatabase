{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from court import Court"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scrape State Court data (past 3 months)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court = Court('supreme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court.pull_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1) Fetch the State Court content by URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target State Court page:\n",
    "url = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=subordinate\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print(response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2) Parse the HTML document with Beautiful Soup.\n",
    "\n",
    "This step allows us to access the elements of the document by XPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_court = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_full = pd.read_csv(f'../data/statecourt_compiled.csv')\n",
    "last_entry = court_full.title[0]\n",
    "last_date = court_full.date[0]\n",
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code collects the date, name, and links for judgments from the High Court \n",
    "# which are accessible on available on LawNet\n",
    "\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "# Create variable for the domain\n",
    "domain = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=1&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=openContentPage&_freeresources_WAR_lawnet3baseportlet_docId=\"\n",
    "\n",
    "# Get number of pages\n",
    "# Remove all extra strings to get the number\n",
    "page = str(state_court.find_all('li', {'class': 'lastPageActive'}))\n",
    "page = ''.join(filter(str.isdigit, page))\n",
    "# Replace the first \"3\" as it comes from the url\n",
    "last_page = int(page.replace(\"3\",\"\",1))\n",
    "\n",
    "# Create counter for current page\n",
    "current_page = 1\n",
    "# Loop while it is not the last page\n",
    "while current_page <= last_page:\n",
    "    url1 = url+\"&_freeresources_WAR_lawnet3baseportlet_page=\"+str(current_page)\n",
    "    # Establishing the connection to the web page:\n",
    "    response1 = requests.get(url1)\n",
    "    # Pull the HTML string out of requests and convert it to a Python string.\n",
    "    html1 = response1.text\n",
    "    state_court1 = BeautifulSoup(html1, 'lxml')\n",
    "    # Get the relevant elements (date, name, link)\n",
    "    search_results = state_court1.find_all('ul', {'class': 'searchResultsHolder'})\n",
    "    for li in search_results:\n",
    "        li_list = li.find_all('li')\n",
    "        li_list.pop(0)\n",
    "        for element in li_list:\n",
    "            # start a dictionary to store this item's data\n",
    "            result = {}\n",
    "            # get the date\n",
    "            date = element.find('p', {'class': 'resultsDate'}).text\n",
    "            result['date'] = date\n",
    "            # get the title and full link/url\n",
    "            a_href = element.find('a')\n",
    "            text = a_href.text\n",
    "            if text == last_entry and date == last_date:\n",
    "                \n",
    "            # only store \"full\" rows of data\n",
    "            elif a_href:\n",
    "                result['title'] = a_href.text.strip()   # element text\n",
    "                link = str(a_href['href']) # href link\n",
    "                link = link.replace(\"javascript:viewContent\",\"\")\n",
    "                link = link.strip(\"')(\")\n",
    "                result['link'] = domain+link\n",
    "            if len(result) == 3:\n",
    "                results_list.append(result)\n",
    "\n",
    "               \n",
    "    # Raise page counter\n",
    "    current_page += 1\n",
    "state_court_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow to only Criminal cases\n",
    "state_court_df = state_court_df[state_court_df['title'].str.contains(\"Public Prosecutor\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1) Fetch the Supreme Court content by URL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Supreme Court page:\n",
    "url = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=supreme\"\n",
    "\n",
    "# Establishing the connection to the web page:\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "# Ex., 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found.\n",
    "print(response.status_code)\n",
    "\n",
    "# Pull the HTML string out of requests and convert it to a Python string.\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2) Parse the HTML document with Beautiful Soup.\n",
    "\n",
    "This step allows us to access the elements of the document by XPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supreme_court = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code collects the date, name, and links for judgments from the High Court \n",
    "# which are accessible on available on LawNet\n",
    "\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "# Create variable for the domain\n",
    "domain = \"https://www.lawnet.sg/lawnet/web/lawnet/free-resources?p_p_id=freeresources_WAR_lawnet3baseportlet&p_p_lifecycle=1&p_p_state=normal&p_p_mode=view&p_p_col_id=column-1&p_p_col_pos=2&p_p_col_count=3&_freeresources_WAR_lawnet3baseportlet_action=openContentPage&_freeresources_WAR_lawnet3baseportlet_docId=\"\n",
    "\n",
    "# Get number of pages\n",
    "# Remove all extra strings to get the number\n",
    "page = str(supreme_court.find_all('li', {'class': 'lastPageActive'}))\n",
    "page = ''.join(filter(str.isdigit, page))\n",
    "# Replace the first \"3\" as it comes from the url\n",
    "last_page = int(page.replace(\"3\",\"\",1))\n",
    "\n",
    "# Create counter for current page\n",
    "current_page = 1\n",
    "# Loop while it is not the last page\n",
    "while current_page <= last_page:\n",
    "    url1 = url+\"&_freeresources_WAR_lawnet3baseportlet_page=\"+str(current_page)\n",
    "    # Establishing the connection to the web page:\n",
    "    response1 = requests.get(url1)\n",
    "    # Pull the HTML string out of requests and convert it to a Python string.\n",
    "    html1 = response1.text\n",
    "    supreme_court1 = BeautifulSoup(html1, 'lxml')\n",
    "    # Get the relevant elements (date, name, link)\n",
    "    search_results = supreme_court1.find_all('ul', {'class': 'searchResultsHolder'})\n",
    "    for li in search_results:\n",
    "        li_list = li.find_all('li')\n",
    "        for element in li_list:\n",
    "            # start a dictionary to store this item's data\n",
    "            result = {}\n",
    "            # get the date\n",
    "            result['date'] = element.find('p', {'class': 'resultsDate'}).text\n",
    "            # get the title and full link/url\n",
    "            a_href = element.find('a')\n",
    "            if a_href:\n",
    "                result['title'] = a_href.text   # element text\n",
    "                link = str(a_href['href']) # href link\n",
    "                link = link.replace(\"javascript:viewContent\",\"\")\n",
    "                link = link.strip(\"')(\")\n",
    "                result['link'] = domain+link\n",
    "            # only store \"full\" rows of data\n",
    "                if len(result) == 3:\n",
    "                    results_list.append(result)\n",
    "    # Raise page counter\n",
    "    current_page += 1\n",
    "supreme_court_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow to only Criminal cases\n",
    "supreme_court_df = supreme_court_df[supreme_court_df['title'].str.contains(\"Public Prosecutor\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compiled databases\n",
    "state_court_full = pd.read_csv('../data/statecourt_compiled.csv')\n",
    "supreme_court_full = pd.read_csv('../data/supremecourt_compiled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with compiled databases and only save new entries\n",
    "state_court_df = state_court_df[~state_court_df['link'].isin(state_court_full['link'])]\n",
    "supreme_court_df = supreme_court_df[~supreme_court_df['link'].isin(supreme_court_full['link'])]\n",
    "\n",
    "state_court_full = state_court_df.merge(state_court_full, how='outer')\n",
    "supreme_court_full = supreme_court_df.merge(supreme_court_full, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to .csv\n",
    "state_court_df.to_csv(path_or_buf=f'../data/statecourt.csv', index=False)\n",
    "supreme_court_df.to_csv(path_or_buf=f'../data/supremecourt.csv', index=False)\n",
    "\n",
    "state_court_full.to_csv(path_or_buf=f'../data/statecourt_compiled.csv', index=False)\n",
    "supreme_court_full.to_csv(path_or_buf=f'../data/supremecourt_compiled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_court_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following Links for More Results\n",
    "\n",
    "One hundred results is pretty good, but what if we want more? We need to follow the \"next\" links and find new pages to grab. Using the **`parse()`** method of our spider class, we need to return another type of object.\n",
    "\n",
    "See [Stack Overflow](https://stackoverflow.com/questions/30152261/make-scrapy-follow-links-and-collect-data) for details!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
